{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "id": "zpFqk0dsCndv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "id": "M3AmheKGBmfe"
   },
   "outputs": [],
   "source": [
    "# Define the start and end dates for the data range\n",
    "start_date = '1985-01-01'\n",
    "end_date = '2018-12-31'\n",
    "\n",
    "# Specify the proportion of test data (e.g., 10%)\n",
    "test_data_size = 0.1\n",
    "\n",
    "# Set the number of splits for K-Fold Cross Validation\n",
    "KFold_split = 5\n",
    "\n",
    "# Specify a range of alpha values for Ridge regression\n",
    "alphas = [10**-4, 10**-3, 10**-2, 10**-1, 10**0, 10**1, 10**2, 10**3, 10**4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "id": "MhbNO9EuIJxx"
   },
   "outputs": [],
   "source": [
    "# Upload X and y data\n",
    "X_mef = pd.read_csv(\"/content/mef_m_clean.csv\")\n",
    "X_mai = pd.read_csv(\"/content/mai_m_clean.csv\")\n",
    "y_mkt = pd.read_csv(\"/content/mkt_m_clean.csv\")\n",
    "\n",
    "X_mef['date'] = pd.to_datetime(X_mef['date'])\n",
    "X_mai['date'] = pd.to_datetime(X_mai['date'])\n",
    "y_mkt['date'] = pd.to_datetime(y_mkt['date'])\n",
    "\n",
    "X_mef = X_mef[(X_mef['date'] >= start_date) & (X_mef['date'] <= end_date)]\n",
    "X_mai = X_mai[(X_mai['date'] >= start_date) & (X_mai['date'] <= end_date)]\n",
    "y_mkt = y_mkt[(y_mkt['date'] >= start_date) & (y_mkt['date'] <= end_date)]\n",
    "\n",
    "# Drop the 'date' column from each dataset\n",
    "X_mef = X_mef.drop('date', axis=1)\n",
    "X_mai = X_mai.drop('date', axis=1)\n",
    "y_mkt = y_mkt.drop('date', axis=1)\n",
    "\n",
    "# Drop the 'GSPCprem' colum from y_mkt\n",
    "y_mkt = y_mkt.drop('GSPCprem', axis=1)\n",
    "\n",
    "X_mef = X_mef.values\n",
    "X_mai = X_mai.values\n",
    "y_mkt = y_mkt.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "id": "oWPidNaOAwys"
   },
   "outputs": [],
   "source": [
    "def split_data(X, y, train_size, indices):\n",
    "    # Split indices into train and test indices\n",
    "    train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "    # Split data based on the indices\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "id": "euuWFg1kA5LL"
   },
   "outputs": [],
   "source": [
    "def find_optimal_ridge_hyperparameters(X_train, y_train, alphas, KFold_split):\n",
    "    optimal_degree = None\n",
    "    optimal_alpha = None\n",
    "    minimal_mse = float('inf')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "    for alpha in alphas:\n",
    "        kf = KFold(n_splits=KFold_split)\n",
    "        mse_arr = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X_train_std):\n",
    "            X_train_n, y_train_n = X_train_std[train_index], y_train[train_index]\n",
    "            X_train_v, y_train_v = X_train_std[test_index], y_train[test_index]\n",
    "\n",
    "            # Train Ridge regression\n",
    "            ridge = Ridge(alpha=alpha)\n",
    "            ridge.fit(X_train_n, y_train_n)\n",
    "\n",
    "            # Predict and calculate MSE on the validation set\n",
    "            y_pred = ridge.predict(X_train_v)\n",
    "            mse = mean_squared_error(y_train_v, y_pred)\n",
    "            mse_arr.append(mse)\n",
    "\n",
    "        # Calculate the average MSE across KFold splits\n",
    "        avg_mse = np.mean(mse_arr)\n",
    "\n",
    "        # Update optimal hyperparameters if the current setup is better\n",
    "        if avg_mse < minimal_mse:\n",
    "            optimal_alpha = alpha\n",
    "            minimal_mse = avg_mse\n",
    "\n",
    "    return optimal_alpha, minimal_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "id": "35YRgMrHDO44"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_ridge(X_train, y_train, X_test, y_test, alpha):\n",
    "    # Standardize both training and test data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "\n",
    "    # Train Ridge regression on the entire training set\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_std, y_train)\n",
    "\n",
    "    # Predict on the training and test sets\n",
    "    y_pred_train = ridge.predict(X_train_std)\n",
    "    y_pred_test = ridge.predict(X_test_std)\n",
    "\n",
    "    # Calculate and print MSE for the training and test sets\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    return mse_train, mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "id": "tJlxSRo3ThGE"
   },
   "outputs": [],
   "source": [
    "def predict_with_ridge(model, scaler, new_X):\n",
    "    # Standardize the new_X using the same scaler used for training\n",
    "    new_X_std = scaler.transform(new_X.reshape(1, -1))\n",
    "\n",
    "    # Predict the target value using the trained Ridge model\n",
    "    predicted_y = model.predict(new_X_std)\n",
    "\n",
    "    return predicted_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "id": "rcdcCJ5VcpxN"
   },
   "outputs": [],
   "source": [
    "# Get same random split of MEF and MAI data\n",
    "N = X_mef.shape[0]\n",
    "train_size = int((1-test_data_size) * N)\n",
    "indices = np.random.permutation(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kmi-PRvuVEzt",
    "outputId": "4853739c-ce2d-4136-8eda-a7277d3150d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEF Linear Predictor\n",
      "Optimal alpha: 100, Minimal MSE: 1259.894677443825\n",
      "Train MSE: 1163.8565285750497\n",
      "Test MSE: 1241.6081749530194\n",
      "\n",
      "new_X:[-3.842472e+00 -3.938737e+00 -2.941030e+00 -9.014420e-01  6.793000e-03\n",
      "  2.555780e-01 -1.921700e-02  2.370000e-02  2.840000e-02  4.810000e-02\n",
      "  4.700000e-03  1.110000e-02 -1.110000e-02 -3.194000e-03]\n",
      "predicted_y: [6.14049578]\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluating Ridge model: MEF\n",
    "print('MEF Linear Predictor')\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, y_train, X_test, y_test = split_data(X_mef, y_mkt, train_size, indices)\n",
    "\n",
    "# Standardize both training and test data using a scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Find optimal alpha using cross-validation\n",
    "optimal_alpha, minimal_mse = find_optimal_ridge_hyperparameters(X_train_std, y_train, alphas, KFold_split)\n",
    "print(f\"Optimal alpha: {optimal_alpha}, Minimal MSE: {minimal_mse}\")\n",
    "\n",
    "# Train and evaluate Ridge model with optimal alpha\n",
    "ridge_model = Ridge(alpha=optimal_alpha)\n",
    "ridge_model.fit(X_train_std, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "mse_train, mse_test = train_and_evaluate_ridge(X_train_std, y_train, X_test_std, y_test, optimal_alpha)\n",
    "print(f\"Train MSE: {mse_train}\")\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(\"\")\n",
    "\n",
    "# Save the trained model and scaler for later use\n",
    "ridge_scaler = scaler\n",
    "\n",
    "# Example of using the trained model for prediction\n",
    "new_X = np.array([-3.842472, -3.938737, -2.941030, -0.901442, 0.006793, 0.255578, -0.019217, 0.0237, 0.0284, 0.0481, 0.0047, 0.0111, -0.0111, -0.003194])\n",
    "predicted_y = predict_with_ridge(ridge_model, ridge_scaler, new_X)\n",
    "\n",
    "print(\"new_X:\" + str(new_X))\n",
    "print(f\"predicted_y: {predicted_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvTb8zmHTnQi",
    "outputId": "b34496de-372f-435f-a3ab-fa1cd237ecf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAI Linear Predictor\n",
      "Optimal alpha: 1000, Minimal MSE: 1258.218615997463\n",
      "Train MSE: 1241.333337949458\n",
      "Test MSE: 1197.106022998405\n",
      "\n",
      "new_X:[0.       0.       0.20982  0.20982  0.       0.481232 1.924928 0.      ]\n",
      "predicted_y: [5.8569198]\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluating Ridge model: MAI\n",
    "print('MAI Linear Predictor')\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, y_train, X_test, y_test = split_data(X_mai, y_mkt, train_size, indices)\n",
    "\n",
    "# Standardize both training and test data using a scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Find optimal alpha using cross-validation\n",
    "optimal_alpha, minimal_mse = find_optimal_ridge_hyperparameters(X_train_std, y_train, alphas, KFold_split)\n",
    "print(f\"Optimal alpha: {optimal_alpha}, Minimal MSE: {minimal_mse}\")\n",
    "\n",
    "# Train and evaluate Ridge model with optimal alpha\n",
    "ridge_model = Ridge(alpha=optimal_alpha)\n",
    "ridge_model.fit(X_train_std, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "mse_train, mse_test = train_and_evaluate_ridge(X_train_std, y_train, X_test_std, y_test, optimal_alpha)\n",
    "print(f\"Train MSE: {mse_train}\")\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(\"\")\n",
    "\n",
    "# Save the trained model and scaler for later use\n",
    "ridge_scaler = scaler\n",
    "\n",
    "# Example of using the trained model for prediction\n",
    "new_X = np.array([0.000000, 0.000000, 0.209820, 0.209820, 0.000000, 0.481232, 1.924928, 0.000000])\n",
    "predicted_y = predict_with_ridge(ridge_model, ridge_scaler, new_X)\n",
    "\n",
    "print(\"new_X:\" + str(new_X))\n",
    "print(f\"predicted_y: {predicted_y}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
